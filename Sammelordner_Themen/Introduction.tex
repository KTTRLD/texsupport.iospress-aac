Comprehensive data collections are indispensable for answering complex scientific questions. In health and medical research, the collection of electronic patient records together with multi-omics data (genomics, epigenomics, transcriptomics, proteomics, metabolomics, interactomics, pharmacogenomics, diseasomics) results in heterogeneous databases that can offer enormous potential for clinicians, researcher and administrators due to the interconnection. 

%%%
Due to the widespread use of relational databases, the most common form of how data is represented and stored and analyzed today is the form of multiple tables linked via primary and foreign keys. In this way the diverse relationships between data values and types, which are important to recognize hidden unknown relationships, are implicit apparent and must be illustrated with the help of additional tools. Visualization tools such as Tableau, i2b2 or KNIME provide data exploration- and analysis functionality for a group of users without explicit programming experience. This software solutions require a certain prior knowledge of the underlying data structure before the analysis can be started. 

Graph databases could allow in addition to the pure data storage additional visual access to the data. Due to the implementation, graph databases are already designed for visual dynamic interaction and enable even inexperienced users to access the data and structures. easy, individual expandability of existing tools, even for typical users who are not experts in statistics or data mining. 

The Medical Information Mart for Intensive Care (MIMIC) is an example of such a large clinical database (DB) containing multiple tables with data of patients who were treated at intensive care units of the Beth Israel Deaconess Medical Center (Boston, Massachusetts, USA) between 2001 and 2012 ~\cite{Johnson.2016}. The third version MIMIC III collects clinical information, about more than 40 000 patients whit information about clinical measurements, administrative processes, laboratory test results, (pharmacological) therapy descriptions, and caregiver notes. ~\cite{Johnson.2016}. To protect the patient's personal information, an online course must be completed in advance and a data usage agreement must be accepted by the researchers who wish to access the data.


\section{State of the art}\label{s2}
To explore and analyze the MIMIC-III Data the 
Visual interactive solutions to enable the analysis and exploration of data and to recognize relationships within the data are diverse. There are already tested solutions that can also be used in the medical field. Tools such as Tableau can be named here as examples ~\cite{Ko.2017} or i2b2 ~\cite{Murphy.2014} or Tableau and i2b2 together~\cite{Harris.2016}. But also tools like RapidMiner, Weka, R tool, KNIME, might be useful in this area. \cite{Dwivedi.18.03.201619.03.2016}



\section{concept}
To interact and explore the MIMIC III data with a Neo4j instance the 26 provided csv files were transformed with a python script which extracted the columns and rows of every file and created the nodes and relationships. In this setting we used the py2neo library to generate the graph database 



\chapter{Struktur der Argumentation}

\paragraph{Klinische Daten können mittels Graph DBs visualisiert werden}
\cite{Stothers.2020} haben gezeigt, dass Neo4j DB für die Representation von klinischen Daten benutzt werden können. Deshalb auch für eine klinische Nutzergrupper interessant, da keine Datenbank Kenntnisse bei der Exploration notwendig sind und eine intuitive Nutzererfahrung möglich wird.

Graphen können als Tool im visuellen Analyseprozess genutzt werden. ~\cite{Landesberger.2011}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Exploration der Daten wird durch Graphen erleichtert}

\section{Textbausteine}

In addition, this data may have to be linked together using multiple joins in advance of the actual analysis. 

The field of visul The amount of data in the healthcare system will continue to grow ~\cite{Murdoch.2013}. 

Representing data in a visually appealing way is not only nice to have, but the way data is represented can also change the perspective and understanding  lead to 

Visual analytics is an emerging discipline that has shown significant promise in addressing many of these information overload challenges.

One goal of visual analytics is to find hidden relations in the data and turn this findings into useful knowledge.

Visual interactive solutions to enable the analysis and exploration of data and to recognize relationships within the data are diverse. There are already established solutions that can also be used in the medical field. Commercial visualization tools such as Tableau can be named here as examples. The providers offer visual solutions from data preparation with transformation to data. Commercial solutions such as Tableau, striim, Disqover, OPTUM, rapidminer to name just a few examples, but also open source solutions such as Knime. One is the structure of the data the. The authors were able to gain most of their experience with Tableau as a visualization tool and analysis tool and would like to view this experience as an exemplary application, since Tableau offers both data preparation options and dynamic exoploration of


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Zitate}

Data analysis is the process of inspecting the data, cleaning the data, transforming the data and modeling data with the goal of discovering useful information, Suggesting conclusions and supporting decision making. 

https://link.springer.com/chapter/10.1007%2F978-3-319-06755-1_16
Die Literatur beschreibt den Visuellen Analyseprozess gut und stellt die entscheidenden schritte dar die man für die generierung von neuen Wissenseinheiten benötigt.

At the first glance, it appears that the analytical reasoning process is a chain of transformations to turn data into understandable knowledge. Yet, this is only half of the truth. The second important goal is the development and verification of these transformations, which also requires human decisions: The selection of data, the choice of analytical methods and models from a large zoo of techniques and the parametrization of these methods are not trivial. Yet, all of these may have severe effects on the outcome of the analysis (May 2011). When searching for new knowledge, we need to bear in mind that the analyst did not make the best choices a priori. Analysis is an iterative, non-linear process and it requires the constant comparison, refinement and possibly the revision of these decisions (Keim et al. 2008).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ideen}
Visuelle Analyse von Daten kann die Erkennung von Mustern und Zusammenhängen in den Daten erleichtern [Beispiele für Tableau, KNIME]. Es gibt mehrere Beispiele die diesen Prozess von den Rohdaten bis zu 


Visualisierungstools können die Erkennung von Mustern und Zusammenhängen in den Daten erleichtern. Es gibt 

\paragraph{warum brauchen wir visual analytics}
Distilling the Right Information: Combining Strengths of Man and Machine
Why visualize analysis? The obvious strengths of automated techniques are the abilities to store, process and retrieve virtually infinite amounts of data. A more subtle nonetheless important advantage of automated methods is that their results are deterministic, repeatable and comparable. This is an indispensable requirement to all scientific and business applications. Traditionally, data analysis has been the domain of statisticians or data mining and machine-learning experts. Ever since computers have existed, they have been used by them to strengthen or refute hypotheses based on empirical data, to identify patterns or to create prediction models from the data. Why do we need to include a new technology to the repertoire of methods?https://link.springer.com/chapter/10.1007%2F978-3-319-06755-1_16

\paragraph{Beispiel warum lineare Correlation nicht ausreicht}
Visualization introduces human strengths into the analysis, which mitigates some fundamental flaws with automated methods. One flaw is that automated methods will never be able to discover something unexpected – by definition. Consider, for example, a technique which is designed to identify all linear correlations between different word count statistics from a large text corpus. We may expect to find linear correlations with this technique because we specifically choose to do so. Selecting another technique will allow us to find another class of patterns or relations, but no single automated technique can claim to find everything that might be relevant. In the end, it remains the responsibility of the human to specify what to search for. https://link.springer.com/chapter/10.1007%2F978-3-319-06755-1_16

\paragraph{automatisierte Methoden werden immer ein ergebnis liefern egal wie gut es ist}
Another flaw is that automated methods will always present a result, regardless of its relevance or quality. Most of them are optimization techniques, which search for the best option from a large but very well-specified set of candidates. To illustrate the problem, imagine you have to pick the best tool from your toolbox to screw something to the wall. If your toolbox contains a large set of candidate screwdrivers, you most likely end up with a good choice. If your toolbox contains a set of candidate hammers, you still might find a “best” one, but still encounter problems in its application.
https://link.springer.com/chapter/10.1007%2F978-3-319-06755-1_16

\paragraph{Data driven vs. hypothesi driven}
Traditionally data analysis has been the domain of statisticians or data mining and machine-learning experts. Their methodologies do not explicitly include visualization. Statisticians typically do confirmatory analysis. Starting with a hypothesis, statistical methods define criteria which can be applied to test data to strengthen or refute the hypothesis. The counterpart of confirmatory analysis is exploratory analysis as introduced by Tukey and Tukey (1985) in the 1970s. Exploratory analysis starts with raw data. The rationale of exploratory analysis is that there might be more interesting patterns and relations in a data collection than a statistician might be able to think of. Its goal is to identify new and useful statements about the data which are worth further investigation. Techniques from data mining and machine-learning are devised to create patterns and prediction models from the data.
https://link.springer.com/chapter/10.1007%2F978-3-319-06755-1_16

\paragraph{Visual Analytics and the Analytical Reasoning Process}
Thomas and Cook (2005) define visual analytics as the “Science of analytical reasoning supported by interactive, visual interfaces”. Analytical reasoning is a process which connects raw data with applicable knowledge via a number of steps of abstraction and refinement. Thomas and Cook call these steps analytical artifacts and distinguish four different levels in the analytical reasoning process (see Fig. 1)
https://link.springer.com/chapter/10.1007%2F978-3-319-06755-1_16

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Big Data und drei "V"}
Big Data wird in der Fachliteratur häufig mit den sogenannten drei „V“ in Verbindung gesetzt: große Datenmengen (Volume), die kaum oder sehr heterogen strukturiert vorliegen (Variety) und die mithilfe mathematisch-statistischer Verfahren und Optimierungsalgorithmen möglichst flexibel und in Echtzeit verarbeitet werden (Velocity). Big Data bezieht sich damit nicht nur auf große Datensammlungen, sondern ebenso auf die angewendeten Auswertungsmethoden, die grundsätzlich nicht hypothesengetrieben sind. 

\paragraph{Beispile für Graphische Datenbanken außer neo4j}
Graph databases such as AllegroGraph, Datastacks and Neo4j are a particularly interesting form of NoSQL database, which store data in graph format as nodes and relationships, focusing on the relationships between nodes as a key feature of the database (Figure 1). 
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7233060/


\prragraph{graph analysis systems}


\section{Visuelle Anayse}
\paragraph{Neo4j als Anwendung für Klinische Nutzergruppe}
In all, Neo4j’s efficient queries and attractive interface lead to ease of use and an intuitive user experience, especially for clinical user without previous database experience

\section{Alle Texte}
#################################################################Gedanken######################################

Researchers are often used to find their data in many distributed tables in the form of rows and columns. In relational databases, for instance, the information is linked across multiple tables by unique identifiers and the relationship between data values is only implicitly apparent. By modelling relationships between data types- or values, graph databases have been used successfully in recent years to represent complex, inhomogeneous data. Furthermore, the exploration of the underlying data structure and the data itself is intuitively possible with graph databases. Neo4j as an example of a graph database system can be adapted in many ways with the help of interactive tools to provide a new way of data exploration and analysis. In the present work, we examine the benefit of graph databases as an exploratory and analytic tool for clinical data. In collaboration with domain experts in Obstetrics and Fetal Medicine, we demonstrate how this resource can improve the efficiency and comprehensiveness of hypothesis generation.

1. Introduction
1.1. Background

3. Implementation

Neo4j mas graph database use labeled 
#####################################################################################################################




### Big Data und Nutzen für die Gesellschaft

> Für die Wissenschaft besitzen große Datensammlungen ein enormes Potenzial zur Beantwortung komplexer Fragestellungen. Die langfristigen Auswirkungen von Ereignissen wie Lärmbelastungen oder eines geringen sozioökonomischen Status in unterschiedlichen Altersstadien auf die Gesundheit im gesamten Lebensverlauf oder die Zusammenhänge zwischen Lebensstil, Genetik und dem Auftreten von Krankheiten wie Demenz oder Krebs sind hier nur zwei von vielen Beispielen. Aus den gewonnenen Ergebnissen lassen sich dann Handlungsempfehlungen und Leitlinien ableiten, die einen direkten Nutzen für die Bevölkerung und das sie behandelnde medizinische Fachpersonal haben. Gleichzeitig ebnen große Datensammlungen den Weg zu einer personalisierten und selbstbestimmteren medizinischen Versorgung. Mithilfe moderner Medien sind die Menschen in der Lage, ihren Gesundheitszustand eigenständig zu kontrollieren. Dabei könnten z. B. individuelle Grenzwerte für Blutdruck oder Blutzucker festgelegt werden. [Pigeot et al.](https://link.springer.com/article/10.1007%2Fs00103-015-2194-6)


### Big Data und drei "V"
Big Data wird in der Fachliteratur häufig mit den sogenannten drei „V“ in Verbindung gesetzt: große Datenmengen (Volume), die kaum oder sehr heterogen strukturiert vorliegen (Variety) und die mithilfe mathematisch-statistischer Verfahren und Optimierungsalgorithmen möglichst flexibel und in Echtzeit verarbeitet werden (Velocity). Big Data bezieht sich damit nicht nur auf große Datensammlungen, sondern ebenso auf die angewendeten Auswertungsmethoden, die grundsätzlich nicht hypothesengetrieben sind. 



## DWH
A data warehouse (DWH) is a complex system that is intended to facilitate the storage and exploration of large amounts of data from multiple sources. Data warehouse (DWH) systems are suitable for the scientific analysis of this clinical data. 

In the present work, we propose a graph-based approach of a DWH to demonstrate the potential benefit of this approach. In collaboration with domain experts in Obstetrics and Fetal Medicine, we demonstrate how this resource can improve the efficiency and comprehensiveness of hypothesis generation. A graph-based approach seems promising for the design of a data warehouse, but the real value of this concept must be continuously evaluated by continuing the work shown. 

\paragraph{DWH erleichterte Datenanalyse}
> The integration and mapping of heterogeneous data sources is an important first step affecting all subsequent analyses for studies in scientific disciplines. The development of the BioDWH2 tool is intended to ease and simplify this process for researchers and to be usable with limited to no programming skills. The hope is that future research projects can focus more quickly on the analysis instead of integration problems using only the specified data sources needed. [Marcel Friedrichs](https://www.degruyter.com/document/doi/10.1515/jib-2020-0033/html)

\paragraph{Nutzen von Data Warehouse}
>This growth in OMICS fields and data sources necessitates research projects to have a reliable and easy to use integration pipeline for data warehousing and information mapping. Additionally, data sources are heterogeneous in format and availability and have a specific focus to which the format is tailored to. A uniform integration process into a singular data format is therefore beneficial. Another issue is the loose coupling of data sources. Identification systems and external references for entities are included in data sources but can’t guarantee that the referenced data sources will not change. Strong mapping of entities in a data warehouse by introducing a mapping layer and connecting entities from different data sources is another important step. Bringing large amounts of data together helps researchers focus on the analyses they want to perform in one place. [Marcel Friedrichs](https://www.degruyter.com/document/doi/10.1515/jib-2020-0033/html)

\subsubsection{Warehouse mit relationalen DBvs graph Data}

The architecture of a data warehouse can differ depending on the application scenarios. Regardless of the architecture, there are components that should be present in a data warehouse system:
•	ETL Component: ETL stands for Extract, Transform, and Load. The staging layer uses ETL tools to extract the needed data from various formats and checks the quality before loading it into the data warehouse. The data coming from the data source layer can come in a variety of formats. Before merging all the data collected from multiple sources into a single database, the system must clean and organize the information.
•	Data storage component: The most crucial component and the heart of each architecture is the database. The warehouse is where the data is stored and accessed. When creating the data warehouse system, you first need to decide what kind of database you want to use. The most dominant form is the relational Database
•	Data Model: The necessary integration of different classes of heterogeneous information in a higher-level, connecting data model is a decisive step in the conception of a data warehouse and one of the decisive factors for the later usability of the system.
•	Data analysis component: Users interact with the gathered information through different tools and technologies. They can analyze the data, gather insight, and create new hypothesis.

\subsubsection{Analysemöglichkeiten für DWH}

- Kohortenselektion
- Kohortenorientierte Analyse
	- Laborverlauf
	- molekulare Befunde
- Kohortenvergleich
	- Kaplan - Meier
	- Patientenliste mit unterschiedlichen 
- Patientenorientierte Analyse
	- Patient Timeline
		- Diagnose
		- Diagnostik
			- Labordiagnostik
			- Apperative Diagnostik
		- Verlegungen
		- Apperative Therapien
		- Medikation
	- Genanalyse
- Patientenvergleich

> "kohortenbasierte Analysen durch Filterung und Kombination von logischen Operatoren (UND / ODER / NICHT) über alle enthaltenen Parameter durchzuführen und die entstehenden Kohorten als Balkendiagramm, Boxplot, Kaplan-Meier-Diagramm, im Genome Browser oder in einer Patientenliste zu visualisieren.Absolute und relative zeitliche Bezüge zwischen einzelnen Datenelementen stellen einen wesentlichen Aspekt bei der Analyse klinisch relevanter Datensätze dar. Beide Ansätze werden von MRI unterstützt. "(DTH- Routinepilot Abschlussbereicht)



\subsection{EHR}

### Electronic Helath records
In the medical field, a wealth of semantically similar clinical data has been collected through the dissemination of electronic health records (EHR).

\subsection{Graph Database}

\subsection{Graphische Datenbanken Nutzen}

In recent years graph databases have demonstrated that they can be used for storing and exploring complex interrelated data in many areas of application. 

> With the increasing and widespread use of graph databases [12], [13], [14], using a relational database for large data warehouses may not be best suited anymore. Especially for an additional mapping layer, the analysis of relational tables with joining queries will become very slow and sometimes unfeasible [15]. While graph databases can outperform relational databases, they also provide the opportunity to reveal novel relationships in heterogeneous data [12]. [Marcel Friedrichs](https://www.degruyter.com/document/doi/10.1515/jib-2020-0033/html)


Graphische Datenbanken haben beweisen, dass sie gerade bei heterogenen Datensätzen zum einen komplexe Verbindungen zwischen Datenelementen speichern und visualisieren können und zum anderen im Vergleich zu aktuell noch vorherrschenden Datenbanksystemen wie Relationalen Datenbanken auch einen Vorteil bezüglich ihrer allgemeinen Performance zeigen. Ferner liefern graphische Datenbanksysteme wie Neo4j umfangreiche Dateninteraktionswerkzeuge, die leicht erweitert werden können und dadurch graphische Datenbanken zu einem Speicher, Explorations- und Analysewerkzeug für den gesamten Datenanalyseprozess werden lassen. Das Konzept der Relationale Datenbanksysteme ist hier enger gedacht und umfasst hauptsächlich die Speicherung und das Datenmanagement in diesem Prozess. Die Aufbereitung, Exploration und Visualisierung verteilt sich hier auf unterschiedliche Teillösungen die sich je nach Präferenz der Nutzer oder den allgemeinen Anforderungen des Fachgebiets unterscheiden können und eine Wiederholung von Forschungsergebnissen von der Vertrautheit mit den Werkzeugen abhängig machen können.  

Der Datenanalyse Prozess der für jeden wissenschaftlich Tätigen und für jede Fachdisziplin individuell unterschiedlich ist setzt aus folgenden generellen Teil abschnitten zusammen, die vom verwendeten Werkzeug zunächst unabhängig sind.  

- Data collection
- Data storage
- Data analytics
- Data reporting and visualization


- Datenspeicherung bzw. Datenmnagement
- Datentransformation
- Datenexploration
- Datenanalyse

In der vorliegenden Arbeit möchten wir an einem konkreten klinischen Datensatz zeigen, dass graphische Datenbanken den gesamten Prozess der Analysen beeinflussen können und die Generierung von Hypothesen bereits durch die Nutzung und Interaktion mit einem graphischen Datenbanksystem ermöglicht wird.


\subsection{Datenanalyse}

### Wie laufen Datenanalysen ab
Raw Data -> 
 
### DWH

Große Datensammlungen sind für die Wissenschaft zur Beantwortung komplexer Fragestellungen sehr wertvoll und besitzen ein großes Potenzial
Graphische Datenbanken konnten in den letzten Jahren demonstrieren, dass sie sich zur Speicherung und Exploration komplexer Daten in vielen Anwendungsbereichen eignen können. Im medizinischen Bereich konnte durch die Verbreitung von electronic health records (EHR) eine fülle von semantisch ähnlichen klinischen Daten gesammelt werden. 
Für die wissenschaftliche Analyse dieser klinischen Daten eignen sich Data warehouse Systeme. In der vorliegenden Arbeit möchten wir die Eigenschaften von Graphischen Datenbanken in einen exemplarischen Data Warehouse Kontext überprüfen.

Ein data warehouse bietet die integrierten Daten, strukturiert und aufbereitet in einem einheitlichen Format zur domainspezifischen Datenanlyse und Exploration an. In der vorliegenden Arbeit wird ein Data Warehouse Ansatz basierend auf einer graphischen Datenbank eingesetzt um die Machbarkeit und 

 sodass sich die Anwender auf die Analyse der Daten fokussieren können.


Ein Data Warehouse stellt eine Plattform dar, welches die Speicherung und die Exploration großer Datenmengen durch spezielle Werkzeuge für eine möglichst große Anwendergruppe erleichtern soll. Wir sind der Meinung, dass graphische Datenbanken zum einen die Speicherung großer Datenmengen als auch die Exploration dieser Daten erleichtert und die Nutzbarkeit und Akzeptanz von Data Warehouses erhöhen k. 

### Daten Model DWH
Die hierzu notwendige Integration unterschiedlicher Klassen heterogener Informationen in ein übergeordnetes verbindendes Datenmodell ist ein entscheidender Schritt in der Konzeption eines Data Warehouses und mitentscheidend für die spätere Nutzbarkeit des Systems. Erfahrungsgemäß sollte ein Datenmodell im Bereich der Medizin oder im Bereich der Naturwissenschaften flexibel und leicht erweiterbar sein. Gleichzeitig sollte das erstellte Datenmodell alle semantischen Informationen der zugrundeliegenden Quellsysteme einschließen ansonsten sind diese für den Anwender in der Exploration nur schwer zu finden und reduzieren das Vertrauen in das System und erschweren die potentielle Nutzbarkeit. 
Anwender sollten einen intuitiven Überblick über das zugrundeliegenden Datenmodell haben um die gewünschten Abfragen optimal zu wählen. In den letzten Jahren zeigte sich, dass speziell graphische Datenbanken sowohl in der Speicherung als auch in der Datenexploration von komplexen zusammenhängenden Daten gut geeignet sein könnten. Mit der vorliegenden Arbeit zeigen wir eine mögliche Realisierung einer Explorationskomponente die für eine Data Warehouse genutzt werden soll und auf einer graphischen Datenbanken basiert. Wir versprechen uns dadurch eine größerer Nutzbarkeit und Akzeptanz des Data Warehouse. Ferner ist die Bereitschaft eigene Datenbestände in einen größeren Kontext einzuordnen.

### Objective and Requirements
Die Exploration von komplexen Daten wird durch das Verständnis der Datenstruktur erleichtert. Relationale Datenbanken basieren auf Verbindungen 
Personen mit Domainwissen bekommen leichteren Zugang zur Datenexploration. Können Hypothesen generieren und Selektionen von Kohorten erleichtern. Kann basis für ein Data Warehouse sein. 
Systemvoraussetzungen der DB. Wie aufgesetzt. Data-Explorer von yworks beschreiben. Vielleicht Tableau als ergänzendes Visualisierungstool beschreiben.


## Conclusion
Wir konnten zeigen, dass sich Graphische Datenbanken in einem Data Warehouse Ansatz in die Datenhaltung, Datenintegration und Datenvisualisierung eingliedern lassen und einen Nutzen für eine breite Anwendergruppe von Domainexperten bringen kann. In weiteren Schritten ist geplant das Data Warehouse für eine große Klinik bereitzustellen und schrittweise zu erweitern.


## Strukturexploration vs Dataexploration
Es soll nicht um die Strukturexploration (anders als [bei )sondern](brain://jhnfj16cFEqSPIMMZN928w/HetionetAnIntegrativeNetworkOfBiomedicalKnowledge) um eine Alternative zur Datenexploration mithilfe von graphischen Datenbanken gehen. Es soll eine schnelle Transformation in eine Graphische Datenbank möglich sein, die dann anschließend visuell exploriert werden kann. Daten werden mit Script in horizontale und vertikale Relationen übersetzt und anschließend mit vorhanden neo4j tools exploriert. Es ist wenig prozessorientiert Kenntnis notwendig und kann ähnlich wie eine Strukturexploration dienen die Daten zu erkunden un



# Einleitung Forschung

Möglichkeiten eine graphische Datenstruktur zu explodieren und dadurch Zusammenhänge explizit ersichtlich zu machen. Durch die Möglichkeit die Interaktion mit Neo4j über spezifische interaktive Werkzeuge zu erweitern könnte  spezielle interaktive Oberflächen ermöglichen es auch ungeübten Nutzern mit der Daten zu interagieren und relevante Zusammenhänge explizit zu erforschen. 

# Graphdata und Neo4j und die Tools




## Test_Introduction
In the medical field, a wealth of semantically similar clinical data has been collected through the dissemination of electronic health records (EHR). 

Die meisten dieser Daten liegen vermutlich in relationalen Datenbanken wie beispielsweise in PostgreSQL und MySQL Datenbanken.



rekonstruierbar. In dieser Struktur zusammenhänge zu erkennen gelingt häufig nach intensicer Ausseinandersetzung und kann sehr zeitaufwendig sein. Graphische Datenbanken eigenen sich Verknüpfungen zwischen Datentypen zu visualisieren und damit explizit der Einzelnen Einheiten häufig  für relationale Datenbanken üblich ist. 

verbunden und müssen für eine spezielle Untersuchung aufbereitet und  Um eine tiefere Analyse durchzuführen zu können und Zusammenhänge zu erkennen, die implizit in den Daten verborgen sind ist die Kenntnis der allgemeinen Datenstruktur notwendig. Die

Die Verknüpfung der einzelner Tabellen erfolgt beispielsweise in relationalen Datenbank architekturen über primary und foreign keys. Um neue Zusammenhänge zu erkennen ist eine Kenntnis der Datenstrukturen und der Datentypen notwendig. Graphische   liegt hierbei in  Die dabei verwendeten Werkzeuge  In dieser Form der Darstellung werde


#####################################################################################################################

## Welche möglichkeiten gibt es für Data analytics
### Data analytics

- Visual Analytics
- Statistical Analytics
	- Regression Analysis:
	It is a statistical tool for investigating the relationship between variables [Artikel](https://www.digitalvidya.com/blog/data-analytics-tools/)

	- Correlation Analysis:
	A statistical technique that allows you to determine whether there is a relationship between two separate variables and how strong that relationship may be. It is best to use when you know or suspect that there is a relationship between two variables and wish to test the assumption.[Artikel](https://www.digitalvidya.com/blog/data-analytics-tools/)

- Time Series Analysis:
	It is the data that is collected at uniformly spaced time intervals. You can use it when you want to assess changes over time or predict future events on the basis of what happened in the past. [Artikel](https://www.digitalvidya.com/blog/data-analytics-tools/)

#####################################################################################################################

\section{Ideen}
Datenvisualisierung, Exploration und Analyse von klinischen Daten mit Neo4j Tools am Beispiel einer klinischen Studie.

- Nachbauen von publizierten Analysen mit graphischen DBs
- welche schritte können mit Neo4j nicht neo4j abgedeckt werden?

- reicht graphische Datenvisualisierung um gesamte Datenanalyse zu modellieren mit den vorhandenen Tools um Zusammenhänge aus Daten schneller zu erkennen.
- Strukturexploration an vielen Beispielen bekannt. Einfaches Datenmodell abgeleitet von relationaler Struktur durch Umwandlung der Spalten in Typen und Zeilen in Pfade.
Funktioniert das? am Beispiel PRINCE. 

######
Big Data stellt eine große unstrukturierte Menge heterogener Daten da, die für die zielgerichtete Suche nach Erkenntnissen oft nur aufwendig zugänglich ist. Methoden des Data Minings stellen statistische Zusammenhänge zwischen zwischen Daten her. Eine hypothesengetriebene Forschung mit relationalen Datensätzen ist zeitaufwendig und nicht immer umsetztbar. Graphische Datenbanken wie beispielsweise Neo4j können zusammenhänge visualisieren und explorierbar machen, dadurch kann für den Anwender eine hypothesengetriebene Analyse möglich werden. Hypothesengetriebe Fragestellungen zielen häufig daurauf ab einen unterschied zwischen einer begrentzen Anzahl klar definierter Kohorten bzgl. interessanter Zielvariablen zu untersuchen. Dies könnte mit de

Wir möchten testen, ob die Transformation von großen relationalen Datenstrukturen mit Hilfe von graphischen Datenbanken in explorierbare Datensätze einen Mehrwert für die Anwender bilden kann. 




### Hypothesengetrieben vs. Nicht-Hypothesengetrieben
-  Hypothesengetriebene Ansätze: --> Wir wollen das an den Daten testen, geht das oder haben wir zu wenig material eher nicht -> Damit unterscheidet sich Big Data von groß angelegten Forschungsdatenbanken, die Daten, in der Regel strukturiert und qualitätsgeprüft, als Basis hypothesengetriebener Forschung bereitstellen.

- Nicht-Hypothesengetrieben Ansätze --> Data-Mining Methoden, die in möglichst kurzer Zeit Korrelationen, also Zusammenhang, in Datenbeständen identifizieren. -> eher ausgeführt auf unterschiedliche, nicht grundsätzlich qualitätsgeprüfte Quellsysteme


### Typen von Datensammlungen

Big-Data: eher ausgeführt auf unterschiedliche, nicht grundsätzlich qualitätsgeprüfte Quellsysteme

Forschung: Damit unterscheidet sich Big Data von groß angelegten Forschungsdatenbanken, die Daten, in der Regel strukturiert und qualitätsgeprüft, als Basis hypothesengetriebener Forschung bereitstellen.

Zwischen diesen beiden Extremen liegen große Datenbanken, die nicht primär zu Forschungszwecken angelegt werden, wie etwa Abrechnungsdaten der Krankenversicherungen, die aber dennoch ein großes Potenzial für die Forschung bieten.

#####################################################################################################################
\
\section{To DO}
 - Klärung yworks und Speicherung der Daten auf Server, falls öffentliche daten dann erstmal egal
 - Mapping PRINCE auf Ontologie SNOMEDCT/Datenstruktur von Het.io
 
 
Markus Scheller Travelmangement
Genehmigter Dienstreiseesteller
040 7410 54251

#####################################################################################################################

\section{Unklare Zuordnung}
Data Warehouse als Basis für Data Analysen. Mit diesem System wollen wir ein konzept für ein Data Warehouse vorstellen, welches auf Graphischen Datenbanken basiert und die Vorteile der Flexiblen Erweiterungen 


In addition to experience and domain knowledge, the analysis of complex, heterogeneous data sets also requires the targeted use of special tools that support the entire process of statistical data preparation and analysis. As experience has shown that every scientist and every discipline uses an individual set of statistical tools, depending on their own experience and preferences, in order to extract valid, reliable, complete and reusable information from semi-structured raw data. 


Even with experienced data experts and scientists, the entire process has a high temporal impact.

Die Analyse komplexer heterogener Datenbestände erfordert neben Erfahrung und Domainkenntnis zusätzlich den gezielten Einsatz spezieller Tools die den gesamten Prozess der statistischen Datenaufbereitung und- Analyse unterstützen. Erfahrungsgemäß nutzt jeder Wissenschaftler und jede Fachdisziplin in Abhängigkeit der eigenen Erfahrungen und Präferenzen einen individuellen Satz an statistischen Werkzeugen um aus semi-strukturierten Rohdaten valide, verlässliche, vollständige und wiederverwendbare Informationen zu extrahieren. Der gesamte Prozess hat selbst bei erfahrenen Datenexperten und Wissenschaftlern einen hohen zeitlichen Einfluss. 

Die zusätzliche Integration eines einzelnen Datensatzes in einen weiteren Datensatz erschwert den Prozess noch weiter.

Ein Data Warehouse kann in diesem Zusammenhang idealerweise diesen Prozess beschleunigen und den Fokus der Analyse auf die Generierung und Überprüfung von Wissen legen. Hierdurch besteht die Hoffnung eine größere Nutzergruppe mit qualitativ hochwertigem Datenmaterial zu versorgen und die domainspezifische Hypothesenbildung zu erleichtern um auf diesem Weg einen Mehrwert aus den Daten zu erzeugen. In einem Data Warehouse, werden die Aspekte der Datenhaltung, Datenkorrektheit, Datenintegration sowie die regelmäßige Aktualisierung vom Endnutzer abstrahiert und in Form einer explorierbaren Datenbasis zur Verfügung gestellt. 


Mit der vorliegenden Arbeit möchten wir einen ersten Schritt in Richtung eines auf graphischen Datenbanken basierenden Data Warehouse Systems legen und eine erste Integration eines klinischen Datensatzes demonstrieren. Wir verwenden hierzu Neo4j als Graphisches Management System und integrieren eine relationale Datenstruktur in ein Graphisches Datenbank Model. Im Anschluss nutzen wir bereits vorhandene Visualisierungsoberflächen, welche die Exploration erleichtern können.


Das Resultat sind auf den Datensatz spezifische  um den eigenen Datenbestand nach relevanten Zusammenhängen zu untersuchen und auf diesem Weg neuen Hypothesen zu generieren und neues Wissen zu erzeugen. 
So sind Statistikprogramme wie SPSS, SAS, GraphPad PRISM und viele weitere, ebenso wie höhere Programmiersprachen wie R oder Python bei der statistischen Datenanalyse nicht wegzudenken. Gleichzeitig werden auch Softwareprodukte wie Knime oder Tableau bei wiederkehrenden Analysen oder als Monitoringinstrument eingesetzt und helfen bei der visualisieren wiederkehrender Fragestellungen [Literatur Tableau und Knime](https://pubmed.ncbi.nlm.nih.gov/29777325/). Ebenso weit verbreitet sind maschinelle Lernverfahren und Methoden der künstlicher Intelligent in der Datenanalyse und Wissensgenerierung die speziell bei großen Datenmengen einen Mehrwert liefern. 

Der Ansatz einer Data Warehouses Plattform ist es, den gesamten skizzierten Prozess von der Datenhaltung über die Datenaufbereitung bis hin zur Datenexploration - und analyse zu abstrahieren und für eine möglichst breite Nutzergruppe anzubieten. Dies soll zu einer Beschleunigung wiederkehrender Analyseabläufe führen und die eigentliche Wissensgenerierung aus den Daten in den Fokus stellen. 

Gleichzeitig kann eine Abstraktion der Abläufe auch dazu führen das erfahrene Nutzergruppen die Nutzbarkeit des Systems als gering einschätzen, da bekannte Funktionen für die erweiterte Datenanalyse fehlen oder anders implementiert sind. 
Ein Data Warehouse sollte damit die grundlegende Funktionalität der etablierten Tools anbieten und gleichzeitig eine potentielle Erweiterbarkeit zur Verfügung stellen.

Unsere Konzeption eines Data Warehouse umfasst folgende Basis Componenten:
- 
- Datenintegration (Übersetzung der Quellsystem Daten in Zielontologie)
- Datapreparation
- Daten- und Strukturexploration
- Datenanalyse mit Datentransformation bzw. Datenerweiterung
- Data and Datastructur Exploration
- Data Visualisierung


Mit dieser Arbeit möchten wir den Einsatz von graphischen Datenbanken und ihren Nutzen als Speicher und Explorationswerkzeug für die Datenexploration untersuchen. Wir glauben, dass graphische Datenbanken das potential bieten den Datenanalyse Prozess zu erleichtern und im Rahmen eines Data Warehouse Projekts zu nutzen.   

Gleichzeitig kann ein Zugang zum Datenpool einer breiteren Nutzergruppe angeboten werden, wodurch die Wissensgenerierung und 


Datenvielfalt und Menge der Daten nimmt zu. Gleichzeitig auch Schwierigkeit dies zu überblicken und sinnvolle Schlüsse zu ziehen. Machine learning kann statistische Korrelationen aufzeigen und unterstützen, jedoch nicht ohne Domain Experten möglich. Graphische Daten liefern zusätzlich Einblicke. Dargestellte Arbeit soll dies unterstreichen.
Aktuellen Datenanalyseprozess skizzieren. Relationale Datenbanken mit Vor\- und Nachteilen beschreiben. Neo4j im Vergleich beschrieben. Beschreibung von Graph Apps als Interaktionsmöglichkeit, Starke Communitiy. Anwendungsgebiete von Neo4j am Beispiel Systembiologie (Reactome), Biologie und Medizin. Fokus auf Relationen der Daten hervorheben. Mit graphischen Datenbanken, können Ontologien ergänzt werden. Graphische Algorithmen sind bereits effizient beschrieben. Können mit Beschreibungslogiken zu Knowlege base ausgebaut werden. Automatisches Schließen möglich.  Unterstützung in der Entscheidungsfindung bei ausreichend großen Daten.
PRINCE Studie und Daten beschrieben. Datenmodell mit Studienzielen grob skizzieren

\chapter{Alles Andere}
Abstract
A Data Warehouse is a system of tools, that collects and combines multiple heterogeneous data and presents them in a comprehensive form to users. The ability to interact and explore this data collection may 



This data collection of big data enables the may lead to new insights and a new hypothesis.  In order to combine and merge multiple data types and sources into one big data source, the conception of a general data model that compromises all semantic aspects of the multiple data sources is an important and complex part of warehouse development. 

The general data model of the Data Warehouse provides the foundation of data interoperability and needs to be compressive and also complete so that every piece of information is easy to find and represented by the model. Every aspect that is captured by the general data model can be queried and accessed by the user. In order to address the right questions to the system, the structure of the data model needs to be present in the head of the users. Often it is also necessary to  



The underling semantic needs to In order to include every data type of the source data set

needed. This general data model might be slightly different from the initial data and implements a new terminology that 

 and needs to be created and necessary This kind of system is a useful tool to for users like researches who want to access the data and do there analysis 

Storing data in multiple tables and connecting these data via multiple-join statements is in most cases the predominant way to start the data analysis process. This approach might sufficient as long as you are familiar with the database schema, but 

Graph databases are getting more and more relevant for storing complex data and their relationships in different fields like medicine or biology. [PROTEOM, Hetionet]. Graph databases like neo4j already demonstrated their power by connecting heterogeneous data in a graph database [Hetionet] and an increasing number of projects starts connecting their data with graph databases. 
In contrast to, currently dominant relational databases like PostgreSQL or MySQL, graph databases are more focused on the connection between the data values and their data types. In addition to the storing function, which makes graphical databases interesting on their own, data exploration is another great opportunity, which graph databases like Neo4j may offer to their users. Furthermore, the great Neo4j community and their already implemented tools make Neo4j even more useful in the sense of exploring patient and clinical data sources. The following work suggests a simple workflow that may empower domain experts to explore their data visually and leverage connection in data to discover new hypothesises or new structures in their data which might be hidden in the table format.


Graph databases already demonstrated their power by connecting heterogeneous data in a graph database. Reactome for instance is a database collecting 

Introduction
The amount and complexity of data in the health industry are constantly raising. Every day a large volume of heterogeneous data is produced by electronic health records. The data compromise many different aspects of the health sector (from physical activities to laboratory tests, and from clinical diagnostics to insurance coverage).


 large More and more data is produced by the routine work of Even complex data is already represented in graphical 
Traditionally data is represented in tabular format for instance in a SQL based database like PostgreSQL. Here the connection between data is made by primary and foreign keys on every table. To analyse the structure of the data you 

Every day, the health industry is producing a large volume of heterogeneous clinical data. Different data types like clinical examinations, laboratory tests, apperativ diagnostics and even insurance data are stored by electronic health records. This amount of data gives us the opportunity to investigate questions and find answers by interpreting the underlying data. In order to make the best use of the data, we need analytic tools to explore these big datasets. Most of the data is stored in relational databases like PostgreSQL or MySQL databases.



The data compromise many different aspects of the health sector (from physical activities to laboratory tests, and from clinical diagnostics to insurance coverage). All of these 

Neo4 j structure
In this work, we used Neo4j as a graph database management system to store and analyze a data set collected from a prospectively designed, longitudinal population-based cohort study called PRINCE study (PRenatal DetermINants of Children’s HEalth). The data was collected from 2010 till now.
A graph is based on two components. The first component is a node with optional properties and optional labels. The second component is a directed relationship or edge with optional properties. Relationships have exactly one start and end node. The start and the end node may be the same node. With these components, it is possible to model complex interactions and relations between data objects. In mathematics, graph theory is used to model structures 

1.2 1 Objective and Requirements
The medical field is based on empirical information. Guidlines statements contain recommendations that are based on evidence from a rigorous systematic review and synthesis of the published medical literature.

Guidelines provided by comites 
A Data Warehouses (DW) for instance connect different data sources together and need to provide every contributor access to the expanded version of his data.  In most cases, the user of the DW wants to query an expanded version of his data. For instance, if someone whats to scan the data for a specific diagnosis or therapy the 

Before a user of Data Warehouses needs to understand the structure of the underlying data to perform well-suited queries and ask the right questions. 

Data
We used clinical data collected from a prospectively designed, longitudinal population-based cohort, the PRINCE study (PRenatal DetermINants of Children’s HEalth). The data was collected from 2010 till now.

Ziel: 

Clinical Data is complex, redundant and semi-structured.
Electronic health records traditionally store clinical data in tabular format in relational database architecture. 
This well-establish process has many advantages like many tools 

Relational Databases need the construct of primary keys and foreign keys to connect the information par

Non-primarily technical user groups like physicians have a lot of domain knowledge but not necessarily the opportunity to analyse data like a data scientist. 
The amount of data is steadily increasing and it is more and more important to link different sources semantically together. 



With this paper, we demonstrate a potential data analytics workflow based on a graph database with the aim to provide an additional way to explore complex heterogeneous data.

Our aim with this work is to create a workflow based on a graph database.

a non-primarily technical user group with an instance of a graph database and Tableau. We used clinical data collected from a prospectively designed, longitudinal population-based cohort, the PRINCE study (PRenatal DetermINants of Children’s HEalth)

Graph databases like neo4j, together with the developed applications for this graph database instance and a broad community offers a big opportunity to develop a visual exploratory toolset for complex data in a new and efficient manner. Data Warehouses for instance need a way to provide the data structure and the underlying metadata to increase the understanding of the data. Ony with an initial understanding of the underlying data structure, the generation of hypothesis or the systematical exploration of the data can be achieved. The representation of data in a tabular format is of 
 


 in comparison to relational databases. In the field of medicine data 





Neo4j as an example of a graph database (graph DB) instance already demonstrated its potential in the context of highly connected, semi-structured and unpredictable data structure. 

as a flexible, intuitive and fast data architecture in the field of biology, chemistry and... The amount and complexity of in comparison to other data storing mechanisms like relational databases to store heterogeneous datasets in the field of biology. Electronic health records (EHR) are traditionally used to store and analyze clinical routine data. 

Relational database architectures are 
Clinical data produced by hospital information systems (HIS) and documented by health care team members will traditionally be stored within relational databases. This type of databases stores the data in a tabular format usually by multiple rows and columns. This structure is well established and capable to save a huge amount of data. It seems like this

neo4j provides a strong community 

Exploration and analysing complex data is a difficult and time-consuming task in the daily routine of researcher and data scientists. Even if the workflow to analyse data and their connections might be well established The task gets even more challenging in the setting of a big data environment with multiple sources of heterogeneous data material. The in the conceptional phase of

With a relational database for instance you need to know the basic structure of your data before you can discover some new attributes. Graph Database could provide a flexible and new insight into the data that you are exploring. Neo4j seems to be a promising instance for graph databases and a great tool to explore data. With simple scripts, you can easily translate the given structure of a relational database into a visually explorable graphical data structure. Where you can obtain new 

Relational databases are well capable to save huge amount of data within a predefined schema. They are well established and offer a 
The complexity of data is 
The use of graphical databases in the clinical praxis is currently limited. Clinical data is stored in relational databases. This approach is well suited and 

